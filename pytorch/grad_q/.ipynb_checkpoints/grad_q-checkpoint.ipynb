{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label counts: [50 50]\n",
      "X.shape: (100, 2)\n",
      "y.shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### DATASET\n",
    "##########################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('toydata.txt', delimiter='\\t')\n",
    "X, y = data[:, :2], data[:, 2]\n",
    "y = y.astype(np.int)\n",
    "\n",
    "print('Class label counts:', np.bincount(y))\n",
    "print('X.shape:', X.shape)\n",
    "print('y.shape:', y.shape)\n",
    "\n",
    "# Shuffling & train/test split\n",
    "shuffle_idx = np.arange(y.shape[0])\n",
    "shuffle_rng = np.random.RandomState(123)\n",
    "shuffle_rng.shuffle(shuffle_idx)\n",
    "X, y = X[shuffle_idx], y[shuffle_idx]\n",
    "\n",
    "X_train, X_test = X[shuffle_idx[:70]], X[shuffle_idx[70:]]\n",
    "y_train, y_test = y[shuffle_idx[:70]], y[shuffle_idx[70:]]\n",
    "\n",
    "# Normalize (mean zero, unit variance)\n",
    "mu, sigma = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level implementation with manual gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def custom_where(cond, x_1, x_2):\n",
    "    return (cond * x_1) + ((1-cond) * x_2)\n",
    "\n",
    "\n",
    "class LogisticRegression1():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = torch.zeros(num_features, 1)\n",
    "        self.bias = torch.zeros(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = torch.add(torch.mm(x, self.weights), self.bias)\n",
    "        probas = self._sigmoid(linear)\n",
    "        return probas\n",
    "        \n",
    "    def backward(self, x, y):  \n",
    "        probas = self.forward(x)\n",
    "        errors = y - probas.view(-1)\n",
    "        return errors\n",
    "            \n",
    "    def predict_labels(self, x):\n",
    "        probas = self.forward(x)\n",
    "        labels = custom_where(probas >= .5, 1, 0)\n",
    "        return labels    \n",
    "            \n",
    "    def evaluate(self, x, y):\n",
    "        labels = self.predict_labels(x).float()\n",
    "        accuracy = torch.sum(labels.view(-1) == y) / y.size()[0]\n",
    "        return accuracy\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1. / (1. + torch.exp(-z))\n",
    "    \n",
    "    def _logit_cost(self, y, proba):\n",
    "        tmp1 = torch.mm(-y.view(1, -1), torch.log(proba))\n",
    "        tmp2 = torch.mm((1 - y).view(1, -1), torch.log(1 - proba))\n",
    "        return tmp1 - tmp2\n",
    "    \n",
    "    def train(self, x, y, num_epochs, learning_rate=0.01):\n",
    "        for e in range(num_epochs):\n",
    "            \n",
    "            print('Epoch: %03d' % (e+1), end=\"\")\n",
    "            errors = self.backward(x, y)\n",
    "            neg_grad = torch.mm(x.transpose(0, 1), errors.view(-1, 1))\n",
    "            self.weights += learning_rate * neg_grad\n",
    "            self.bias += learning_rate * torch.sum(errors)\n",
    "            print(' | Train ACC: %.3f' % self.evaluate(x, y), end=\"\")\n",
    "            print(' | Cost: %.3f' % self._logit_cost(y, self.forward(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train ACC: 0.971 | Cost: 4.986\n",
      "Epoch: 002 | Train ACC: 0.971 | Cost: 4.469\n",
      "Epoch: 003 | Train ACC: 0.971 | Cost: 4.114\n",
      "Epoch: 004 | Train ACC: 0.971 | Cost: 3.862\n",
      "Epoch: 005 | Train ACC: 0.971 | Cost: 3.673\n",
      "Epoch: 006 | Train ACC: 0.971 | Cost: 3.525\n",
      "Epoch: 007 | Train ACC: 0.971 | Cost: 3.404\n",
      "Epoch: 008 | Train ACC: 0.971 | Cost: 3.301\n",
      "Epoch: 009 | Train ACC: 0.971 | Cost: 3.210\n",
      "Epoch: 010 | Train ACC: 0.986 | Cost: 3.128\n",
      "\n",
      "Model parameters:\n",
      "  Weights: \n",
      " 3.5362\n",
      " 3.0126\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "  Bias: \n",
      "-1.4038\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression1(num_features=2)\n",
    "X_train_tensor, y_train_tensor = torch.Tensor(X_train), torch.Tensor(y_train)\n",
    "logr.train(X_train_tensor, y_train_tensor, num_epochs=10, learning_rate=0.1)\n",
    "\n",
    "print('\\nModel parameters:')\n",
    "print('  Weights: %s' % logr.weights)\n",
    "print('  Bias: %s' % logr.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 100.000%\n"
     ]
    }
   ],
   "source": [
    "X_test_tensor, y_test_tensor = torch.Tensor(X_test), torch.Tensor(y_test)\n",
    "\n",
    "test_acc = logr.evaluate(X_test_tensor, y_test_tensor)\n",
    "print('Test set accuracy: %.3f%%' % (test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level implementation using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def custom_where(cond, x_1, x_2):\n",
    "    return (cond * x_1) + ((1-cond) * x_2)\n",
    "\n",
    "\n",
    "class LogisticRegression2():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = Variable(torch.zeros(num_features, 1).type(dtype),\n",
    "                                requires_grad=True)\n",
    "        self.bias = Variable(torch.zeros(1).type(dtype),\n",
    "                             requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = torch.add(torch.mm(x, self.weights), self.bias)\n",
    "        probas = self._sigmoid(linear)\n",
    "        return probas\n",
    "                    \n",
    "    def predict_labels(self, x):\n",
    "        probas = self.forward(x)\n",
    "        labels = custom_where((probas >= .5).float(), 1, 0)\n",
    "        return labels    \n",
    "            \n",
    "    def evaluate(self, x, y):\n",
    "        labels = self.predict_labels(x)\n",
    "        accuracy = (torch.sum(labels.view(-1) == y.view(-1))).float() / y.size()[0]\n",
    "        return accuracy\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1. / (1. + torch.exp(-z))\n",
    "    \n",
    "    def _logit_cost(self, y, proba):\n",
    "        tmp1 = torch.mm(-y.view(1, -1), torch.log(proba))\n",
    "        tmp2 = torch.mm((1 - y).view(1, -1), torch.log(1 - proba))\n",
    "        return tmp1 - tmp2\n",
    "    \n",
    "    def train(self, x, y, num_epochs, learning_rate=0.01):\n",
    "        \n",
    "        x_var = Variable(x.type(dtype), requires_grad=False)\n",
    "        y_var = Variable(y.type(dtype), requires_grad=False)\n",
    "        \n",
    "        for e in range(num_epochs):\n",
    "            \n",
    "            print('Epoch: %03d' % (e+1), end=\"\")\n",
    "            proba = self.forward(x_var)\n",
    "            cost = self._logit_cost(y_var, proba)\n",
    "            cost.backward()\n",
    "            \n",
    "            self.weights.data -= learning_rate * self.weights.grad.data\n",
    "            self.bias.data -= learning_rate * self.bias.grad.data\n",
    "            \n",
    "            print(' | Train ACC: %.3f' % self.evaluate(x_var, y_var), end=\"\")\n",
    "            print(' | Cost: %.3f' % self._logit_cost(y_var, self.forward(x_var)))\n",
    "            \n",
    "            self.weights.grad.data.zero_()\n",
    "            self.bias.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train ACC: 0.971 | Cost: 4.986\n",
      "Epoch: 002 | Train ACC: 0.971 | Cost: 4.469\n",
      "Epoch: 003 | Train ACC: 0.971 | Cost: 4.114\n",
      "Epoch: 004 | Train ACC: 0.971 | Cost: 3.862\n",
      "Epoch: 005 | Train ACC: 0.971 | Cost: 3.673\n",
      "Epoch: 006 | Train ACC: 0.971 | Cost: 3.525\n",
      "Epoch: 007 | Train ACC: 0.971 | Cost: 3.404\n",
      "Epoch: 008 | Train ACC: 0.971 | Cost: 3.301\n",
      "Epoch: 009 | Train ACC: 0.971 | Cost: 3.210\n",
      "Epoch: 010 | Train ACC: 0.986 | Cost: 3.128\n",
      "\n",
      "Model parameters:\n",
      "  Weights: Variable containing:\n",
      " 3.5362\n",
      " 3.0126\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "  Bias: Variable containing:\n",
      "-1.4038\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression2(num_features=2)\n",
    "X_train_tensor, y_train_tensor = torch.Tensor(X_train), torch.Tensor(y_train)\n",
    "logr.train(X_train_tensor, y_train_tensor, num_epochs=10, learning_rate=0.1)\n",
    "\n",
    "print('\\nModel parameters:')\n",
    "print('  Weights: %s' % logr.weights)\n",
    "print('  Bias: %s' % logr.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "X_test_var = Variable(torch.Tensor(X_test).type(dtype), requires_grad=False)\n",
    "y_test_var = Variable(torch.Tensor(y_test).type(dtype), requires_grad=False)\n",
    "\n",
    "test_acc = logr.evaluate(X_test_var, y_test_var)\n",
    "print('Test set accuracy: %.2f%%' % (test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Module API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression3(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super(LogisticRegression3, self).__init__()\n",
    "        self.linear = torch.nn.Linear(num_features, 1)\n",
    "        # initialize weights to zeros here:\n",
    "        self.linear.weight.data.zero_()\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        probas = F.sigmoid(logits)\n",
    "        return probas\n",
    "\n",
    "model = LogisticRegression3(num_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_fn = torch.nn.BCELoss(size_average=False)\n",
    "# set size_average=False to match the manual approach\n",
    "#   since there, we didn't normalize the loss by the \n",
    "#   number of training examples\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train ACC: 0.971 | Cost: 4.986\n",
      "Epoch: 002 | Train ACC: 0.971 | Cost: 4.469\n",
      "Epoch: 003 | Train ACC: 0.971 | Cost: 4.114\n",
      "Epoch: 004 | Train ACC: 0.971 | Cost: 3.862\n",
      "Epoch: 005 | Train ACC: 0.971 | Cost: 3.673\n",
      "Epoch: 006 | Train ACC: 0.971 | Cost: 3.525\n",
      "Epoch: 007 | Train ACC: 0.971 | Cost: 3.404\n",
      "Epoch: 008 | Train ACC: 0.971 | Cost: 3.301\n",
      "Epoch: 009 | Train ACC: 0.971 | Cost: 3.210\n",
      "Epoch: 010 | Train ACC: 0.986 | Cost: 3.128\n",
      "\n",
      "Model parameters:\n",
      "  Weights: \n",
      " 3.5362  3.0126\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "  Bias: \n",
      "-1.4038\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def comp_accuracy(label_var, pred_probas):\n",
    "    pred_labels = custom_where((pred_probas > 0.5).float(), 1, 0).view(-1)\n",
    "    acc = torch.sum(pred_labels == label_var.view(-1)).float() / label_var.size(0)\n",
    "    return acc\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "X_train_var = Variable(torch.Tensor(X_train), requires_grad=False)\n",
    "y_train_var = Variable(torch.Tensor(y_train), requires_grad=False).view(-1, 1)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    out = model(X_train_var)\n",
    "    cost = cost_fn(out, y_train_var)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch: %03d' % (epoch + 1), end=\"\")\n",
    "    pred_probas = model(X_train_var)\n",
    "    acc = comp_accuracy(y_train_var, pred_probas)\n",
    "    print(' | Train ACC: %.3f' % acc, end=\"\")\n",
    "    print(' | Cost: %.3f' % cost_fn(pred_probas, y_train_var))\n",
    "\n",
    "\n",
    "    \n",
    "print('\\nModel parameters:')\n",
    "print('  Weights: %s' % model.linear.weight.data)\n",
    "print('  Bias: %s' % model.linear.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "X_test_var = Variable(torch.Tensor(X_test).type(dtype), requires_grad=False)\n",
    "y_test_var = Variable(torch.Tensor(y_test).type(dtype), requires_grad=False)\n",
    "\n",
    "pred_probas = model(X_test_var)\n",
    "test_acc = comp_accuracy(y_test_var, pred_probas)\n",
    "\n",
    "print('Test set accuracy: %.2f%%' % (test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
